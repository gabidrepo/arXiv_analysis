{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceType":"datasetVersion","sourceId":8248756,"datasetId":612177,"databundleVersionId":8376528}],"dockerImageVersionId":30664,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import json\nimport re\n\ndataset = \"/kaggle/input/arxiv/arxiv-metadata-oai-snapshot.json\"","metadata":{"_uuid":"60e30f2a-c6c3-446e-ab06-75561471be2f","_cell_guid":"a69c8c52-a54f-49d9-8aae-483483073d9b","collapsed":false,"execution":{"iopub.status.busy":"2024-04-21T06:11:26.454303Z","iopub.execute_input":"2024-04-21T06:11:26.454662Z","iopub.status.idle":"2024-04-21T06:11:26.459554Z","shell.execute_reply.started":"2024-04-21T06:11:26.454636Z","shell.execute_reply":"2024-04-21T06:11:26.458373Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\n\narxiv_df = pd.read_json(dataset, lines=True)\nprint(len(arxiv_df))\narxiv_df.drop(columns=['journal-ref', 'doi','report-no','submitter','license','comments'], inplace=True)\narxiv_df.dropna(inplace=True)\nprint(len(arxiv_df))","metadata":{"_uuid":"02a158e7-e314-4563-99ff-c1c29861b36e","_cell_guid":"772b070a-00ed-42bf-8f0e-131ac36fcdb0","collapsed":false,"execution":{"iopub.status.busy":"2024-04-21T06:11:32.163097Z","iopub.execute_input":"2024-04-21T06:11:32.163812Z","iopub.status.idle":"2024-04-21T06:14:20.317951Z","shell.execute_reply.started":"2024-04-21T06:11:32.163779Z","shell.execute_reply":"2024-04-21T06:14:20.316693Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print (arxiv_df.columns)","metadata":{"_uuid":"7928993d-9f62-4546-8e7c-951cfbdc8854","_cell_guid":"e1d11eb1-2026-451b-9947-ec348d860eff","collapsed":false,"execution":{"iopub.status.busy":"2024-04-09T18:26:11.488686Z","iopub.execute_input":"2024-04-09T18:26:11.489256Z","iopub.status.idle":"2024-04-09T18:26:11.495604Z","shell.execute_reply.started":"2024-04-09T18:26:11.489217Z","shell.execute_reply":"2024-04-09T18:26:11.494606Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":" arxiv_df","metadata":{"_uuid":"b17d92f3-0811-4408-8048-deb2467bc14e","_cell_guid":"bb9234bf-4afb-4605-9649-ca8c84411976","collapsed":false,"execution":{"iopub.status.busy":"2024-04-09T17:13:57.940477Z","iopub.execute_input":"2024-04-09T17:13:57.940768Z","iopub.status.idle":"2024-04-09T17:13:57.989716Z","shell.execute_reply.started":"2024-04-09T17:13:57.940745Z","shell.execute_reply":"2024-04-09T17:13:57.988871Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def count_authors(authors_list):\n    return len(authors_list)\n\narxiv_df['num_authors'] = arxiv_df['authors_parsed'].apply(count_authors)\narxiv_df","metadata":{"_uuid":"feab4875-6335-4c6e-b569-6f872b5f22b5","_cell_guid":"c8ca75e3-1fce-4787-9288-930956cfbba4","collapsed":false,"execution":{"iopub.status.busy":"2024-04-21T06:14:20.335033Z","iopub.execute_input":"2024-04-21T06:14:20.335465Z","iopub.status.idle":"2024-04-21T06:14:22.334184Z","shell.execute_reply.started":"2024-04-21T06:14:20.335435Z","shell.execute_reply":"2024-04-21T06:14:22.333121Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n\n# Convert 'update_date' to datetime format\narxiv_df['update_date'] = pd.to_datetime(arxiv_df['update_date'])\n\n# Extract year from the 'update_date' column\narxiv_df['year'] = arxiv_df['update_date'].dt.year\n\n# Group by year and calculate the average author count\naverage_author_count_by_year = arxiv_df.groupby('year')['num_authors'].mean().reset_index()\n\n# Plot using Seaborn\nplt.figure(figsize=(10, 6))\nsns.barplot(x='year', y='num_authors', data=average_author_count_by_year, color='skyblue',zorder=0)\n\n# sns.regplot(x='year', y='num_authors', data=average_author_count_by_year, scatter=False, color='red')\nsns.despine(offset=10, trim=False)\nplt.title('Average authors count by year')\nplt.xlabel('Year')\nplt.ylabel('Average author count')\nplt.xticks(rotation=45)\nplt.grid(axis='y')  # Add gridlines on y-axis\nplt.tight_layout()\nplt.savefig('average_authors_count_by_year.png')\nplt.show()","metadata":{"_uuid":"bdd6bf20-ca8a-46be-9c4a-41fa7b791f11","_cell_guid":"0a43ef32-3b7c-404e-9654-79a76be75acd","collapsed":false,"execution":{"iopub.status.busy":"2024-04-21T06:14:22.335443Z","iopub.execute_input":"2024-04-21T06:14:22.336357Z","iopub.status.idle":"2024-04-21T06:14:25.822357Z","shell.execute_reply.started":"2024-04-21T06:14:22.336323Z","shell.execute_reply":"2024-04-21T06:14:25.821179Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def count_versions(versions_list):\n    return len(versions_list)\n\narxiv_df['num_versions'] = arxiv_df['versions'].apply(count_versions)\n\narxiv_df","metadata":{"_uuid":"b8c9964c-a5dd-448c-a178-6c58c707b180","_cell_guid":"117f92ed-a006-4add-bc8c-a24586a421d6","collapsed":false,"execution":{"iopub.status.busy":"2024-04-15T16:54:56.652743Z","iopub.execute_input":"2024-04-15T16:54:56.653198Z","iopub.status.idle":"2024-04-15T16:54:57.983194Z","shell.execute_reply.started":"2024-04-15T16:54:56.653157Z","shell.execute_reply":"2024-04-15T16:54:57.982060Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Group by year and calculate the average verisons count\naverage_versions_count_by_year = arxiv_df.groupby('year')['num_versions'].mean().reset_index()\n\n# Plot using Seaborn\nplt.figure(figsize=(10, 6))\nsns.barplot(x='year', y='num_versions', data=average_versions_count_by_year, color='skyblue')\nplt.title('Average versions count by year')\nplt.xlabel('Year')\nplt.ylabel('Average Vvrsions count')\nplt.xticks(rotation=45)\nplt.grid(axis='y')  # Add gridlines on y-axis\nplt.tight_layout()\nplt.savefig('average_versions_count_by_year.png')\nplt.show()","metadata":{"_uuid":"332f905b-46ab-4f10-8498-6bb52f4b612c","_cell_guid":"8282a60f-79a6-4430-9be1-cb40a48e2b87","collapsed":false,"execution":{"iopub.status.busy":"2024-04-15T16:56:13.371170Z","iopub.execute_input":"2024-04-15T16:56:13.371593Z","iopub.status.idle":"2024-04-15T16:56:14.090646Z","shell.execute_reply.started":"2024-04-15T16:56:13.371562Z","shell.execute_reply":"2024-04-15T16:56:14.088834Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install beautifulsoup4\n!pip install requests","metadata":{"_uuid":"ad8163eb-2143-4aaa-9b01-39b6d5ad5902","_cell_guid":"90bd847d-5b0b-48d5-bfe4-fcfdc41ae746","collapsed":false,"execution":{"iopub.status.busy":"2024-04-15T16:55:04.644471Z","iopub.execute_input":"2024-04-15T16:55:04.644950Z","iopub.status.idle":"2024-04-15T16:55:31.805854Z","shell.execute_reply.started":"2024-04-15T16:55:04.644893Z","shell.execute_reply":"2024-04-15T16:55:31.804533Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import requests\nfrom bs4 import BeautifulSoup\n\n# Send an HTTP GET request to the URL\nresponse = requests.get(\"https://arxiv.org/category_taxonomy\")\n\n# Check if the request was successful (status code 200)\nif response.status_code == 200:\n    # Parse the HTML content\n    soup = BeautifulSoup(response.content, 'html.parser')\n    \n    # Find the div containing the category taxonomy\n    taxonomy_div = soup.find('div', id='category_taxonomy_list')\n    \n    # Initialize an empty dictionary to store the sub-category and its father category\n    category_dict = {}\n    \n    # Find all h2 tags (father categories)\n#     father_categories = taxonomy_div.find_all(['h2','h3'], class_='accordion-head')\n    father_categories_h2 = taxonomy_div.find_all('h2', class_='accordion-head')\n    father_categories_h3 = taxonomy_div.find_all('h3', class_='column is-one-fifth')\n    \n    # Combine both lists of father categories\n    father_categories = father_categories_h2 + father_categories_h3\n    \n    # Iterate over each father category\n    for father_category in father_categories:\n        # Extract father category name\n        father_category_name = father_category.text.strip()\n        \n        # Find all h4 tags (sub-categories) within the current father category\n        sub_categories = father_category.find_next_sibling().find_all('h4')\n#         sub_categories = father_category.find_next_sibling().find_all(['h3', 'h4'])\n\n        # Iterate over each sub-category\n        for sub_category in sub_categories:\n            # Extract sub-category name\n            sub_category_name = sub_category.text.strip()\n            \n            # Extract sub-category code\n            sub_category_code = sub_category_name.split()[0]\n            \n            # Add the sub-category and its father category to the dictionary\n            category_dict[sub_category_code] = father_category_name\n\n    # Display the dictionary\n    print(category_dict)\nelse:\n    # Display an error message if the request was not successful\n    print(\"Failed to retrieve webpage content. Status code:\", response.status_code)","metadata":{"_uuid":"8eb5f529-749b-4f81-a88e-337042e33eb8","_cell_guid":"073d7f43-c5c8-4500-b59e-440a1e57293f","collapsed":false,"execution":{"iopub.status.busy":"2024-04-21T06:14:25.825363Z","iopub.execute_input":"2024-04-21T06:14:25.825747Z","iopub.status.idle":"2024-04-21T06:14:26.534062Z","shell.execute_reply.started":"2024-04-21T06:14:25.825715Z","shell.execute_reply":"2024-04-21T06:14:26.532930Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import requests\nfrom bs4 import BeautifulSoup\n\n# URL of the website\nurl = \"https://arxiv.org/category_taxonomy\"\n\n# Send an HTTP request to the URL\nresponse = requests.get(url)\n\n# Check if the request was successful (status code 200)\nif response.status_code == 200:\n    # Parse the HTML content using BeautifulSoup\n    soup = BeautifulSoup(response.text, 'html.parser')\n\n    # Find all relevant HTML elements (all <h4> elements)\n    category_elements = soup.find_all(['h3','h4'])\n\n    # Create a dictionary to store key-value pairs\n    categories_dict = {}\n\n    # Extract information and populate the dictionary\n    for category_element in category_elements:\n        category_key = category_element.text.strip().split()[0]\n        \n        # Check if there is a <span> element\n        span_element = category_element.find('span')\n        if span_element:\n            category_value = span_element.text.strip()\n        else:\n            category_value = \"No Value Found\"\n        \n        if (category_value!=\"No Value Found\"):\n            categories_dict[category_key] = category_value.strip(')').strip('(')\n\n    # Print the key-value pairs\n    for key, value in categories_dict.items():\n        print(f\"Key: {key}\")\n        print(f\"Value: {value}\")\n        print()\n\nelse:\n    print(f\"Failed to retrieve data. Status code: {response.status_code}\")","metadata":{"_uuid":"9c6949fd-55bf-40c3-b870-12461947219e","_cell_guid":"e48f5997-c859-43cf-b007-4e163adbc119","collapsed":false,"execution":{"iopub.status.busy":"2024-04-21T06:14:26.536401Z","iopub.execute_input":"2024-04-21T06:14:26.537538Z","iopub.status.idle":"2024-04-21T06:14:26.719607Z","shell.execute_reply.started":"2024-04-21T06:14:26.537447Z","shell.execute_reply":"2024-04-21T06:14:26.718745Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"arxiv_df['sep_categories'] = arxiv_df['categories'].str.split()\narxiv_df= arxiv_df[arxiv_df['year']<2024]\n# Explode the list into separate rows\ndf_exploded = arxiv_df.explode('sep_categories')\n\n# Create a new column with the corresponding values from categories_dict\ndf_exploded['category'] = df_exploded['sep_categories'].str.strip().map(categories_dict)\n\n# Display the updated DataFrame\nprint(len(df_exploded))\n\n# arxiv_df['CategoryValue'] = arxiv_df['categories'].map(categories_dict)\n# arxiv_df\ndf_exploded.dropna(inplace=True)\ndf_exploded","metadata":{"_uuid":"4841274e-6c93-491e-b2dd-b462ad87e3d5","_cell_guid":"f0b2aac6-ea31-4513-8d04-a3319bff449e","collapsed":false,"execution":{"iopub.status.busy":"2024-04-21T06:14:26.721196Z","iopub.execute_input":"2024-04-21T06:14:26.721562Z","iopub.status.idle":"2024-04-21T06:14:45.821960Z","shell.execute_reply.started":"2024-04-21T06:14:26.721531Z","shell.execute_reply":"2024-04-21T06:14:45.820811Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"arxiv_df['count_topics'] = arxiv_df['categories'].apply(lambda x: len(x.split()))\narxiv_df","metadata":{"_uuid":"63d1fd69-e49d-467b-be2c-3ec8dbdfa782","_cell_guid":"f555a458-56a6-4ff4-adbf-657078610861","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"average_topic_count_by_year = arxiv_df.groupby('year')['count_topics'].mean().reset_index()\n\n# Plot using Seaborn\nplt.figure(figsize=(10, 6))\nsns.barplot(x='year', y='count_topics', data=average_topic_count_by_year, color='skyblue',zorder=0)\nplt.title('Average categories count by year')\nplt.xlabel('Year')\nplt.ylabel('Average categories count')\nplt.xticks(rotation=45)\nplt.grid(axis='y')  # Add gridlines on y-axis\nplt.tight_layout()\nplt.savefig('average_categories_count_by_year.png')\nplt.show()","metadata":{"_uuid":"d2dbba0f-6040-4068-844c-3bbc4f4cd5d0","_cell_guid":"52fac402-8e54-42d6-83c1-0f16164f59f0","collapsed":false,"execution":{"iopub.status.busy":"2024-04-09T18:29:49.591685Z","iopub.execute_input":"2024-04-09T18:29:49.592862Z","iopub.status.idle":"2024-04-09T18:29:50.580940Z","shell.execute_reply.started":"2024-04-09T18:29:49.592821Z","shell.execute_reply":"2024-04-09T18:29:50.579444Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(average_topic_count_by_year)","metadata":{"_uuid":"791435ad-cbfe-4549-afd4-df81f343aa9a","_cell_guid":"cdcf22ae-67a7-4274-ab7a-51930fbeb653","collapsed":false,"execution":{"iopub.status.busy":"2024-04-09T17:15:12.558313Z","iopub.execute_input":"2024-04-09T17:15:12.558605Z","iopub.status.idle":"2024-04-09T17:15:12.565994Z","shell.execute_reply.started":"2024-04-09T17:15:12.558580Z","shell.execute_reply":"2024-04-09T17:15:12.564921Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_exploded['top_category'] = df_exploded['sep_categories'].apply(lambda x: category_dict[x])\ndf_exploded","metadata":{"_uuid":"930ac7db-9647-4b56-a62b-4332cf127575","_cell_guid":"8b6b6007-3568-4128-be0b-d45696f44547","collapsed":false,"execution":{"iopub.status.busy":"2024-04-21T06:14:45.823552Z","iopub.execute_input":"2024-04-21T06:14:45.824769Z","iopub.status.idle":"2024-04-21T06:14:47.460641Z","shell.execute_reply.started":"2024-04-21T06:14:45.824714Z","shell.execute_reply":"2024-04-21T06:14:47.459552Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# Group by 'year' and 'category' and count the number of publications\npublications_by_category_year = df_exploded.groupby(['year', 'top_category']).size().reset_index(name='count')\npublications_by_category_year['year'] = publications_by_category_year['year'].astype(str)\n\n# Plot using Seaborn\nsns.set(style=\"whitegrid\")\nplt.figure(figsize=(12, 8))\nsns.lineplot(x='year', y='count', hue='top_category', data=publications_by_category_year,markers=True, dashes=False,lw=2)\nplt.title('Number of publications by category each year')\nplt.xlabel('Year')\nplt.ylabel('Number of publications')\nplt.legend(title='Category', loc='upper left')\nplt.xticks(rotation=45)\nplt.grid(axis='y')  # Add gridlines on y-axis\nplt.tight_layout()\nplt.savefig('Number_of_Publications_by_Category_by_Year.png')\nplt.show()","metadata":{"_uuid":"b441dd94-1d92-4079-baa2-b0bdc4a6ecd9","_cell_guid":"7a2d552c-7999-4bf7-a5a3-5a40eb6af72d","collapsed":false,"execution":{"iopub.status.busy":"2024-04-09T18:30:01.625585Z","iopub.execute_input":"2024-04-09T18:30:01.626060Z","iopub.status.idle":"2024-04-09T18:30:03.687948Z","shell.execute_reply.started":"2024-04-09T18:30:01.626023Z","shell.execute_reply":"2024-04-09T18:30:03.686438Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"connected_cat = df_exploded[['id','category','year']]\n\nconnected_cat","metadata":{"_uuid":"d9c1d3c4-5a1a-4306-9edd-6d588b4e1cf8","_cell_guid":"31c320dd-62d9-46fc-a624-a1fefd618040","collapsed":false,"execution":{"iopub.status.busy":"2024-04-09T18:30:10.451076Z","iopub.execute_input":"2024-04-09T18:30:10.451569Z","iopub.status.idle":"2024-04-09T18:30:10.726403Z","shell.execute_reply.started":"2024-04-09T18:30:10.451535Z","shell.execute_reply":"2024-04-09T18:30:10.725086Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"publication_counts = connected_cat.groupby(['year', 'category']).size().reset_index(name='count')\npublication_counts_sorted = publication_counts.sort_values(by=['year', 'count'], ascending=[True, False])\ntop_categories_each_year = publication_counts_sorted.groupby('year').head(3).reset_index()\ntop_10_categories_each_year = publication_counts_sorted.groupby('year').head(10).reset_index()\ntop_10_categories_each_year['category'].replace(\" - \", ' ', regex=True, inplace=True)\ntop_categories_each_year['category'].replace(\" - \", ' ', regex=True, inplace=True)\n\n# Pivot the data to create stacked bar plot\npivot_df = top_categories_each_year.pivot(index='year', columns='category', values='count')\n\n# Plot stacked bar plot\n# plt.figure(figsize=(18, 10))\npivot_df.plot(kind='bar', stacked=True, colormap='Set3', width=0.8,figsize=(12, 6))\nplt.title('Top 3 categories each year')\nplt.xlabel('Year')\nplt.ylabel('Number of publications')\nplt.xticks(rotation=45)\nplt.legend(title='Legend title', loc='upper left')  # Adjust legend position as needed\n\nplt.tight_layout()\nplt.savefig('Top 3 categories each year.png')\nplt.show()","metadata":{"_uuid":"a6f6b192-abab-4c08-bb17-4332bfaaa922","_cell_guid":"ab4c2d32-c63b-4c4e-8da2-9f3488b85729","collapsed":false,"execution":{"iopub.status.busy":"2024-04-09T18:30:13.628723Z","iopub.execute_input":"2024-04-09T18:30:13.629241Z","iopub.status.idle":"2024-04-09T18:30:15.958801Z","shell.execute_reply.started":"2024-04-09T18:30:13.629195Z","shell.execute_reply":"2024-04-09T18:30:15.957547Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(top_categories_each_year)","metadata":{"_uuid":"ba5e39e7-41af-4171-ac76-395e4609545b","_cell_guid":"eeddcd3c-afd1-41f0-b455-f6036757bd13","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"top_10_categories_each_year","metadata":{"_uuid":"b3089a2e-3360-4cb4-b80f-968cb5dfb6e7","_cell_guid":"82242391-164e-4535-9aa6-842ca2f47f03","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# !pip install -U kaleido\n# !pip install fiftyone -y","metadata":{"_uuid":"4ef4e68d-b852-490e-9f9f-31f3d5e04551","_cell_guid":"0969b346-70db-4234-b463-1c1febcd78d5","collapsed":false,"execution":{"iopub.status.busy":"2024-04-29T20:00:04.045953Z","iopub.execute_input":"2024-04-29T20:00:04.046361Z","iopub.status.idle":"2024-04-29T20:00:04.051215Z","shell.execute_reply.started":"2024-04-29T20:00:04.046329Z","shell.execute_reply":"2024-04-29T20:00:04.050001Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install plotly\n!conda install -c conda-forge python-kaleido -y","metadata":{"_uuid":"8fb23fa3-c06a-4527-9417-64602933e715","_cell_guid":"4e5cc4fc-f1b9-4255-a9e6-874b56b1c0a3","collapsed":false,"execution":{"iopub.status.busy":"2024-04-21T06:14:47.462017Z","iopub.execute_input":"2024-04-21T06:14:47.462307Z","iopub.status.idle":"2024-04-21T06:17:35.434448Z","shell.execute_reply.started":"2024-04-21T06:14:47.462283Z","shell.execute_reply":"2024-04-21T06:17:35.432772Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import plotly.graph_objects as go\nimport plotly.io as pio\nimport plotly.express as px\n\ntop_10_categories_each_year['v_cat'] = top_10_categories_each_year['category'].str.replace(' ', '<br>')\n\nfig = go.Figure()\nfig = px.treemap(top_10_categories_each_year, path=['year', 'v_cat'], values='count', title='Treemap of Categories by Year',branchvalues='total', width=1600, height=1200)\nfig.update_traces(textinfo='label+value', selector=dict(type='treemap'),textfont_size=12, pathbar_textfont_size=12)\nfig.show()\nfig.write_image(\"test1.png\",scale=6, width=1600, height=900)","metadata":{"_uuid":"0dace336-b085-4a30-b440-20a59ca989de","_cell_guid":"82ae0b24-7083-4308-88d1-cbdc423a8182","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"connected_cat","metadata":{"_uuid":"fff9969d-9de0-4563-acc7-9507c636168e","_cell_guid":"4d489dfd-2211-4e11-ab57-5b0bfca111a7","collapsed":false,"execution":{"iopub.status.busy":"2024-04-09T17:18:09.410015Z","iopub.execute_input":"2024-04-09T17:18:09.410391Z","iopub.status.idle":"2024-04-09T17:18:09.423704Z","shell.execute_reply.started":"2024-04-09T17:18:09.410364Z","shell.execute_reply":"2024-04-09T17:18:09.422635Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"most_common_category = connected_cat.groupby('year')['category'].agg(lambda x: x.mode()[0]).reset_index()\nmost_common_category\nPlot using Seaborn\nplt.figure(figsize=(10, 6))\nsns.barplot(x='year', y='category', data=most_common_category, palette='viridis')\nplt.title('Most Common Category Each Year')\nplt.xlabel('Year')\nplt.ylabel('Category')\nplt.xticks(rotation=45)\nplt.tight_layout()\nplt.show()","metadata":{"_uuid":"299dfda0-a8f8-4325-985b-3cf77aa9e001","_cell_guid":"49cf925c-14b9-4c2b-9a27-b2eae7597cdc","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"network_connected_cat = connected_cat.groupby('id')","metadata":{"_uuid":"b799b136-1696-48a8-b084-693bf66ec631","_cell_guid":"7658cfc1-a3e3-4c6a-ab54-6a646832005a","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import networkx as nx\nimport plotly.graph_objects as go\nfrom itertools import combinations\n\n# Create an empty graph\nG = nx.Graph()\n\nfor idx in list(network_connected_cat.groups.keys()):\n    cat_len = len(network_connected_cat.get_group(idx))\n    if(cat_len>1):\n        pairs = list(combinations(list(range(0, cat_len)), 2))\n        for pair in pairs:\n            category1=network_connected_cat.get_group(idx).iloc[pair[0]].category\n            category2=network_connected_cat.get_group(idx).iloc[pair[1]].category\n            G.add_edge(category1, category2)\n\n# Calculate betweenness centrality for each node\nnode_betweenness = nx.betweenness_centrality(G)\n\n# Sort nodes based on betweenness centrality and select the top 100\ntop_nodes = sorted(G.nodes(), key=lambda x: node_betweenness[x], reverse=True)[:40]\n\n# Filter edges to include only those connected to the top nodes\nfiltered_edges = [(source, target) for source, target in G.edges() if source in top_nodes or target in top_nodes]\n\n# Create a subgraph containing only the top nodes and their connected edges\nG_sub = G.subgraph(top_nodes)\n\n# Create positions for the nodes using a spring layout algorithm with a higher k value for a more sparse layout\npos_sub = nx.spring_layout(G_sub, k=2)\n\n# Create edge trace for the filtered edges\nedge_x = []\nedge_y = []\nfor edge in G_sub.edges():\n    x0, y0 = pos_sub[edge[0]]\n    x1, y1 = pos_sub[edge[1]]\n    edge_x.extend([x0, x1, None])\n    edge_y.extend([y0, y1, None])\n\n# Create node trace for the top nodes\nnode_x = []\nnode_y = []\nnode_text = []\nnode_size = []\nfor node in G_sub.nodes():\n    x, y = pos_sub[node]\n    node_x.append(x)\n    node_y.append(y)\n    node_text.append(node)\n    node_size.append(node_betweenness[node])\n\n# Define the scaling factor and adjust the size range\nscaling_factor = 10000\nmin_size = 3\nmax_size = 100\n\n# Scale the node sizes based on the betweenness centrality\nscaled_node_size = [min_size + (s * scaling_factor) for s in node_size]\nscaled_node_size = [min(s, max_size) for s in scaled_node_size]\n\n# Create edge trace\nedge_trace = go.Scatter(\n    x=edge_x,\n    y=edge_y,\n    line=dict(width=0.5, color='#888'),\n    hoverinfo='none',\n    mode='lines'\n)\n\n# Create node trace\nnode_trace = go.Scatter(\n    x=node_x,\n    y=node_y,\n    text=node_text,\n    mode='markers+text',\n    hoverinfo='none',\n    marker=dict(\n        color='#CB1111',\n        size=scaled_node_size,\n        line=dict(width=2)\n    )\n)\n\n# Create figure\nfig = go.Figure(\n    data=[edge_trace, node_trace],\n    layout=go.Layout(\n        title='Top 40 Connected Categories',\n        titlefont=dict(size=8),\n        showlegend=False,\n        hovermode='closest',\n        margin=dict(b=20, l=5, r=5, t=40),\n        xaxis=dict(showgrid=False, zeroline=False, showticklabels=True),\n        yaxis=dict(showgrid=False, zeroline=False, showticklabels=True)\n    )\n)\n\n# Show the figure\nplt.show()","metadata":{"_uuid":"971a7e70-cd40-43b2-a203-e2a796881128","_cell_guid":"a640e2dc-fe0f-4e9f-abc3-783abe5aff50","collapsed":false,"execution":{"iopub.status.busy":"2024-04-09T17:18:10.135101Z","iopub.execute_input":"2024-04-09T17:18:10.135416Z","iopub.status.idle":"2024-04-09T17:42:07.274884Z","shell.execute_reply.started":"2024-04-09T17:18:10.135392Z","shell.execute_reply":"2024-04-09T17:42:07.273849Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pio.write_image(fig, 'top_40_connected_categories.png',width=1100, height=700, scale=6)","metadata":{"_uuid":"759f640e-2abb-4767-b358-317afa1685d6","_cell_guid":"cccfb321-962a-4c32-b559-1fbbf6d001ca","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Sort nodes based on betweenness centrality values\nsorted_nodes = sorted(node_betweenness.items(), key=lambda x: x[1], reverse=True)\n\n# Select top 10 nodes\ntop_10_nodes = sorted_nodes[:10]\n\n# Extract node labels and betweenness centrality values\nnode_labels = [node[0] for node in top_10_nodes]\ncentrality_values = [node[1] for node in top_10_nodes]\n\n# Plot bar graph\nplt.figure(figsize=(10, 6))\nplt.bar(node_labels, centrality_values, color='lightblue')\nplt.title('Top 10 Categoires of Betweenness Centrality')\nplt.xlabel('Category')\nplt.ylabel('Betweenness Centrality')\nplt.xticks(rotation=45)\nplt.tight_layout()\n\n# Save the figure\nplt.savefig('top_10_nodes_betweenness_centrality.png')\n\n# Show the figure\nplt.show()","metadata":{"_uuid":"62c8d147-59d0-4d9f-a2d5-46b57e8a37e2","_cell_guid":"fffa1a39-a052-4985-9fb5-632b1eac0ff0","collapsed":false,"execution":{"iopub.status.busy":"2024-04-09T17:45:57.077870Z","iopub.execute_input":"2024-04-09T17:45:57.078772Z","iopub.status.idle":"2024-04-09T17:45:57.813621Z","shell.execute_reply.started":"2024-04-09T17:45:57.078735Z","shell.execute_reply":"2024-04-09T17:45:57.812670Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install bertopic\n!pip install nltk","metadata":{"_uuid":"1d42b338-a53d-4808-b410-0cf4d0412d97","_cell_guid":"c0a23d1d-6ce8-445f-a2a5-fa642511c8c0","collapsed":false,"execution":{"iopub.status.busy":"2024-04-21T06:17:35.437012Z","iopub.execute_input":"2024-04-21T06:17:35.437437Z","iopub.status.idle":"2024-04-21T06:19:27.002434Z","shell.execute_reply.started":"2024-04-21T06:17:35.437398Z","shell.execute_reply":"2024-04-21T06:19:27.001315Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_exp_cs = df_exploded[df_exploded['category']=='Machine Learning']\ndf_exp_cs = df_exp_cs[['id','abstract','year','category']]\n\nprint(len(df_exp_cs))","metadata":{"_uuid":"e4e91f5a-2fcb-494a-be85-a4a154ea2e68","_cell_guid":"17353097-1025-4e17-b916-4bd103b4378a","collapsed":false,"execution":{"iopub.status.busy":"2024-04-21T06:19:27.008862Z","iopub.execute_input":"2024-04-21T06:19:27.009242Z","iopub.status.idle":"2024-04-21T06:19:29.052879Z","shell.execute_reply.started":"2024-04-21T06:19:27.009208Z","shell.execute_reply":"2024-04-21T06:19:29.051651Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_exp_cs","metadata":{"_uuid":"7a2fb41e-3eed-4f50-b92e-a9b4c8c1c1a8","_cell_guid":"8ee4f711-1129-4e57-a331-48175d39a3b7","collapsed":false,"execution":{"iopub.status.busy":"2024-04-19T09:22:47.793683Z","iopub.execute_input":"2024-04-19T09:22:47.794003Z","iopub.status.idle":"2024-04-19T09:22:47.810636Z","shell.execute_reply.started":"2024-04-19T09:22:47.793976Z","shell.execute_reply":"2024-04-19T09:22:47.809248Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install BERTopic","metadata":{"_uuid":"7b8b0710-ef1f-4bb5-b099-86ef1671c758","_cell_guid":"509dc545-34ed-4cbd-a536-59f206f710f5","collapsed":false,"execution":{"iopub.status.busy":"2024-04-19T05:11:33.313123Z","iopub.execute_input":"2024-04-19T05:11:33.313787Z","iopub.status.idle":"2024-04-19T05:13:15.759494Z","shell.execute_reply.started":"2024-04-19T05:11:33.313748Z","shell.execute_reply":"2024-04-19T05:13:15.758003Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import nltk\nfrom nltk.tokenize import word_tokenize\nfrom nltk.corpus import stopwords\nfrom bertopic import BERTopic\nimport vaex\n\n# Download NLTK resources\nnltk.download('punkt')\nnltk.download('stopwords')\n\n# Tokenize the abstracts and remove stop words using NLTK\nstop_words = set(stopwords.words('english'))\n\ndef preprocess_text(text):\n    tokens = word_tokenize(text.lower())\n    tokens = [token for token in tokens if token.isalpha()]\n    tokens = [token for token in tokens if token not in stop_words]\n    return ' '.join(tokens)\n\n# Apply preprocessing to the abstract column and convert to veax df\nvaex_arxiv_df = vaex.from_pandas(df_exp_cs)\nvaex_arxiv_df['cleaned_abstract'] = vaex_arxiv_df['abstract'].apply(preprocess_text)","metadata":{"_uuid":"ca1278e5-1b54-433a-aee1-e192060b0eb3","_cell_guid":"cb970bdd-73c3-444c-a44c-a2f08d55b2fa","collapsed":false,"_kg_hide-output":true,"execution":{"iopub.status.busy":"2024-04-21T06:19:29.054370Z","iopub.execute_input":"2024-04-21T06:19:29.054749Z","iopub.status.idle":"2024-04-21T06:20:21.246932Z","shell.execute_reply.started":"2024-04-21T06:19:29.054718Z","shell.execute_reply":"2024-04-21T06:20:21.245762Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Convert the cleaned abstracts to a list\nabstracts_list = vaex_arxiv_df['cleaned_abstract'].tolist()","metadata":{"_uuid":"1a5fb842-5e93-4525-9c52-ce9d557ece3c","_cell_guid":"50d80049-cc43-477a-8027-ed4618108e4f","collapsed":false,"execution":{"iopub.status.busy":"2024-04-21T06:20:21.248460Z","iopub.execute_input":"2024-04-21T06:20:21.249701Z","iopub.status.idle":"2024-04-21T06:24:15.167784Z","shell.execute_reply.started":"2024-04-21T06:20:21.249665Z","shell.execute_reply":"2024-04-21T06:24:15.165569Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.decomposition import LatentDirichletAllocation\nfrom collections import Counter\nimport matplotlib.pyplot as plt\n\n# Assuming is your Pandas DataFrame with the 'cleaned_abstract' column\n\n\n# Vectorize the text data\nvectorizer = CountVectorizer(max_features=1000, stop_words='english')\nX = vectorizer.fit_transform(abstracts_list)\n\n# Apply online LDA\nnum_topics = 10  \nbatch_size = 1000  \nlda_model = LatentDirichletAllocation(n_components=num_topics, learning_method='online', random_state=42, batch_size=batch_size)\nlda_model.fit(X)\n\n# Get the most common words for each topic\ntopic_words = []\nfeature_names = vectorizer.get_feature_names_out()\nfor topic_idx, topic in enumerate(lda_model.components_):\n    top_words_idx = topic.argsort()[:-11:-1]\n    topic_words.append([feature_names[i] for i in top_words_idx])","metadata":{"_uuid":"7653a8de-1db5-4eb4-aff7-f25bb66b6c0f","_cell_guid":"80b5ced3-a93d-48d0-a8c7-177d22bf068b","collapsed":false,"execution":{"iopub.status.busy":"2024-04-16T16:25:38.730495Z","iopub.execute_input":"2024-04-16T16:25:38.732166Z","iopub.status.idle":"2024-04-16T16:52:14.849433Z","shell.execute_reply.started":"2024-04-16T16:25:38.732115Z","shell.execute_reply":"2024-04-16T16:52:14.846003Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nimport matplotlib.pyplot as plt\n\n# Flatten the list of top words for all topics\nall_topic_words = [word for topic in topic_words for word in topic]\n\n# Count the frequency of each word\nword_counts = Counter(all_topic_words)\n\n# Sort the words by frequency\nsorted_words = sorted(word_counts.items(), key=lambda x: x[1], reverse=True)\n\n# Extract the top 20 words and their frequencies\ntop_words = [word[0] for word in sorted_words[:20]]\nword_frequencies = [word[1] for word in sorted_words[:20]]\n\n# Plot the horizontal bar chart\nplt.figure(figsize=(10, 8))\nplt.barh(top_words, word_frequencies, color='skyblue')\nplt.xlabel('Frequency')\nplt.ylabel('Words')\nplt.title('Top common words across all topics in machine learning')\nplt.gca().invert_yaxis()  # Invert y-axis to display the most frequent words at the top\nplt.savefig('Most_common_topics_in_Machine_Learing.png')\nplt.show()","metadata":{"_uuid":"028bfdfa-8768-4ad5-9d5a-83d3e15feb3b","_cell_guid":"d1078df3-4e72-4cdf-a162-9e7461b1971b","collapsed":false,"execution":{"iopub.status.busy":"2024-04-16T17:08:48.048100Z","iopub.execute_input":"2024-04-16T17:08:48.049144Z","iopub.status.idle":"2024-04-16T17:08:48.913690Z","shell.execute_reply.started":"2024-04-16T17:08:48.049060Z","shell.execute_reply":"2024-04-16T17:08:48.912072Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from bertopic import BERTopic\nimport random\n\n\nrandom.shuffle(abstracts_list)\n\n# Select a random subset of 125,000 items\nsubset_abstracts_list = random.sample(abstracts_list, 10000)\n\n# Initialize and fit the BERTopic model with the subset\ntopic_model = BERTopic(verbose=True, embedding_model=\"paraphrase-MiniLM-L12-v2\", min_topic_size=50)\ntopics, _ = topic_model.fit_transform(subset_abstracts_list)\n\n# Get information about the topics\ntopic_info = topic_model.get_topic_info()\nprint(len(topic_info))  # Check the number of topics","metadata":{"_uuid":"c5021ed6-0601-4d62-a0e2-4f86246c7eee","_cell_guid":"274a6f75-c27c-49d7-9ca2-1edde46a5c8e","collapsed":false,"execution":{"iopub.status.busy":"2024-04-21T06:24:15.170842Z","iopub.execute_input":"2024-04-21T06:24:15.171275Z","iopub.status.idle":"2024-04-21T06:36:17.678287Z","shell.execute_reply.started":"2024-04-21T06:24:15.171231Z","shell.execute_reply":"2024-04-21T06:36:17.677160Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"topic_model.get_topic_info().head(10)","metadata":{"_uuid":"f6b016c2-5056-4351-87fc-f22b09659684","_cell_guid":"2d94a77b-4f54-4c6b-8428-4310defc16d4","collapsed":false,"execution":{"iopub.status.busy":"2024-04-21T05:53:31.785383Z","iopub.execute_input":"2024-04-21T05:53:31.786703Z","iopub.status.idle":"2024-04-21T05:53:31.819845Z","shell.execute_reply.started":"2024-04-21T05:53:31.786661Z","shell.execute_reply":"2024-04-21T05:53:31.818647Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"topic_info = topic_model.get_topic_info().head(10)  # Get the top 10 topic information\n\n# Save the topic information to a CSV file\ntopic_info.to_csv(\"topic_info.csv\", index=False)","metadata":{"_uuid":"71baa442-8f9d-4b5e-9733-2ef5a81cbbbc","_cell_guid":"1b20cb37-f5f1-46d9-be31-f9105c33a092","collapsed":false,"execution":{"iopub.status.busy":"2024-04-21T06:36:17.679650Z","iopub.execute_input":"2024-04-21T06:36:17.679954Z","iopub.status.idle":"2024-04-21T06:36:17.702209Z","shell.execute_reply.started":"2024-04-21T06:36:17.679928Z","shell.execute_reply":"2024-04-21T06:36:17.701452Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"topic_model.visualize_barchart(top_n_topics=11)","metadata":{"_uuid":"a681ca7e-592a-4c0a-b6b4-c53dd3be433a","_cell_guid":"465d5e31-f485-499a-b8f3-94742964c49b","collapsed":false,"execution":{"iopub.status.busy":"2024-04-21T05:54:37.729243Z","iopub.execute_input":"2024-04-21T05:54:37.729708Z","iopub.status.idle":"2024-04-21T05:54:37.864040Z","shell.execute_reply.started":"2024-04-21T05:54:37.729674Z","shell.execute_reply":"2024-04-21T05:54:37.862874Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Assuming topic_model is already initialized and fitted\nbarchart = topic_model.visualize_barchart(top_n_topics=11)\n\n# Save the visualization as an image file\nbarchart.write_image(\"topic_barchart.png\", width=1920, height=1080)","metadata":{"_uuid":"befa8727-8775-4ea2-88be-e5777436163c","_cell_guid":"8242eb93-8814-46b7-9af2-a385cd388716","collapsed":false,"execution":{"iopub.status.busy":"2024-04-21T06:59:49.945613Z","iopub.execute_input":"2024-04-21T06:59:49.946094Z","iopub.status.idle":"2024-04-21T06:59:50.611884Z","shell.execute_reply.started":"2024-04-21T06:59:49.946059Z","shell.execute_reply":"2024-04-21T06:59:50.610726Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"topic_model.visualize_topics()","metadata":{"_uuid":"7de89671-8317-4154-a436-fc92ee41490a","_cell_guid":"56651693-04ce-4094-aa28-56a4839ee798","collapsed":false,"execution":{"iopub.status.busy":"2024-04-21T07:00:42.859778Z","iopub.execute_input":"2024-04-21T07:00:42.860114Z","iopub.status.idle":"2024-04-21T07:00:44.148712Z","shell.execute_reply.started":"2024-04-21T07:00:42.860085Z","shell.execute_reply":"2024-04-21T07:00:44.147608Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"visualization = topic_model.visualize_topics()\n\n# Save the visualization as an image file with high resolution (e.g., 1920x1080)\nvisualization.write_image(\"topic_visualization.png\", width=1920, height=1080)","metadata":{"_uuid":"0c8a7234-dee8-4ec3-9daa-5a433f72c574","_cell_guid":"b56908e1-e91e-4aae-920e-978d2bbd58ee","collapsed":false,"execution":{"iopub.status.busy":"2024-04-21T07:01:27.788959Z","iopub.execute_input":"2024-04-21T07:01:27.789441Z","iopub.status.idle":"2024-04-21T07:01:29.271013Z","shell.execute_reply.started":"2024-04-21T07:01:27.789409Z","shell.execute_reply":"2024-04-21T07:01:29.269818Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"topic_info = topic_model.get_topic_info()\n\n# Save the data to a text file\ntopic_info.to_csv(\"topic_data.txt\", sep='\\t', index=False)","metadata":{"_uuid":"f068ddd2-a5fc-4b57-be63-8f05ca8ba6bd","_cell_guid":"14a3de02-af1d-487b-93a6-3d5b4af64c6f","collapsed":false,"execution":{"iopub.status.busy":"2024-04-21T07:06:20.137839Z","iopub.execute_input":"2024-04-21T07:06:20.138456Z","iopub.status.idle":"2024-04-21T07:06:20.157894Z","shell.execute_reply.started":"2024-04-21T07:06:20.138421Z","shell.execute_reply":"2024-04-21T07:06:20.156763Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.decomposition import LatentDirichletAllocation\nimport matplotlib.pyplot as plt\n\n# Assuming df is your Pandas DataFrame with the 'cleaned_abstract' column\n# Convert the cleaned abstracts to a list\n# abstracts_list = df['cleaned_abstract'].tolist()\n\n# Vectorize the text data using TF-IDF\nvectorizer = TfidfVectorizer(max_features=1000, stop_words='english')\nX = vectorizer.fit_transform(abstracts_list)\n\n# Apply LDA\nnum_topics = 10  # Adjust as needed\nlda_model = LatentDirichletAllocation(n_components=num_topics, random_state=42)\nlda_model.fit(X)\n\n# Get the TF-IDF scores for each term within each topic category\ntopic_term_tfidf = lda_model.components_ * lda_model.components_.max(axis=1)[:, np.newaxis]\n\n# Plot the Term TF-IDF distribution per assigned topic category\nplt.figure(figsize=(12, 8))\nfor topic_idx in range(num_topics):\n    sorted_indices = topic_term_tfidf[topic_idx].argsort()[::-1][:10]\n    sorted_terms = [vectorizer.get_feature_names_out()[idx] for idx in sorted_indices]\n    sorted_scores = topic_term_tfidf[topic_idx, sorted_indices]\n    plt.barh([f\"Topic {topic_idx}\"] * 10, sorted_scores, tick_label=sorted_terms, alpha=0.7)\nplt.xlabel('TF-IDF Score')\nplt.ylabel('Terms')\nplt.title('Term TF-IDF Distribution per Assigned Topic Category')\nplt.savefig('tf-idf.png')\n\nplt.show()","metadata":{"_uuid":"c2a6070f-2f10-462d-b9d0-4d4b23fda9b6","_cell_guid":"b0b58567-6e0c-4435-8343-c9d904fd4529","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Topics models","metadata":{"_uuid":"b6c19853-1eaf-4f28-a492-ad61992b1ebd","_cell_guid":"cfd79581-d82e-40d3-81ac-6f9b31489a7e","trusted":true}}]}